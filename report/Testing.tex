%% ----------------------------------------------------------------
%% Testing.tex
%% ---------------------------------------------------------------- 

\chapter{Testing} \label{Chapter: Testing}

\begin{preamble}
Software testing is necessary on projects of all sizes, especially on large projects such as this. To ensure quality throughout the project each part of the framework has been thoroughly tested. This chapter details different testing methodologies along with the results of the tests.
\end{preamble}

\section{Introduction}

During development number of testing methods were used for each section of the project to reflect the different types of module. All sections that have requirements that directly specify features such as \cref{Req:Keyboard accessibility} will have user acceptance tests performed. These tests directly form part of the deliverables report (\cref{App:Deliverable Report}).

A number of sections have a deliverable sign-off table. While working in an agile method a working program that could be presented to the client at any time was kept. This was usually done during the client/supervisor meetings. During final tests number deliverable sign-off tests were created that were used to prove to the customer that the software worked correctly. These were presented in the deliverable report (see \cref{App:Deliverable Report}) that was presented to the client and signed off.

These deliverable sign-off tests were used as manual regression tests. These were used during the final changes and bug fixes to ensure the functionality that the client wished were still present and working correctly.

\section{Tools}

A number and range of different tools have been used during testing to ensure that the quality of the code is to the high standard the customer required. For each repository the types of tests that are applicable were decided and can be implemented to properly test the application.

\subsection{Unit Testing Plan}
\label{Section:Testing_unittesting}

Where possible unit test style tools have been used to automate repeatable types of tests. Anyone working with a repository that has unit tests should follow the following policies:

\begin{itemize}
\item \textbf{Running unit tests after checking out new code} - This is to ensure that all code checked out is currently working. If it does not they are encouraged to look into fixing the problem themselves or determining which commit broke the build and contact the committer to find the reason for this.

\item \textbf{Create unit tests for new functionality} - The unit tests form part of the continuous testing plan with the aim to ensure all code on master branches runs properly. Adding unit tests for new features helps ensure this as others will be able to run them to check your new features are working correctly after other code modifications have been made.

\item \textbf{Run unit tests before committing new code} - As a final check before submitting new code, all unit tests should be run on the repository. This acts as a final confirmation to ensure that no new code has broken old code. This forms part of the regression testing. Any older code that has been broken must be reviewed and fixed to ensure that the master branch runs properly.

\item \textbf{Fix broken unit tests} - When a unit test is identified that it is not working as intended it should be reviewed to see if it is still applicable and ensure that it is modified to work if so. There may be multiple reasons why a unit test has been broken so it may be needed to contact the person who originally broke the unit test to find out what was changed and why.
\end{itemize}

The tools that were used in the testing sections below are described fully in the following subsections.

\subsection{JSHint}

JSHint is a tool for static code analysis of JavaScript, which can detect many instances of bad coding practice in source files before they cause errors. Included in each of the repositories containing JavaScript is a configuration file for JSHint, which enforces use of ECMAScript 5 Strict Mode, and prohibits use of undeclared variables or type coercion. JSHint was run at regularly during the project and any issues it highlighted were fixed. For repositories that had JSHint set up for use it was recommended to run this before committing.

\subsection{Karma}

Karma allows tests to be run in browsers (or headlessly) in an automated manner.  It then also aggregates and displays the results making it easy to see if the tests have passed or failed.

Karma is included within Angular seed (see \autoref{Section:Overall_Testing}), which means it was available from the start. The inclusion in Angular seed was one of the reasons it was used as it is an industry standard for AngularJS.

\subsection{Jasmine}

Jasmine is a testing framework for JavaScript. It has simple syntax and works well with Karma.

Jasmine was also included in Angular seed (see \autoref{Section:Overall_Testing}) and therefore already set up for use.

This has been used as the unit test framework for AngularJS code within the project.

\subsection{Python unittest module}

Python has included in its standard packages, a module called \lstinline|unittest|. This module is designed to be similar to the JUnit testing framework for Java and this has been used for all python based projects as a unit test framework.

\subsection{Locust}

Locust\footnote{\url{http://locust.io/}} is a distributed load testing tool that allows definition of a site access pattern. By designing a site access pattern you specify how often sites are visited relative to others and therefore can tailor the access patterns to your likely use case.

This tool has been used as it is able to give percentile data to see statistics on how fast all requests are fulfilled. Many other similar tools only give an average request time which will cause problems when some users are kept waiting for a much longer time. Having access to a full set of statistics allows the running of performance checks and improvement of features so no users are left waiting.

\section{Videogular Questions Example} 
\label{Section:Videogular Questions Example}

The Videogular Questions Example site was written as testing and documentation for how Videogular Questions, Videogular Cuepoints and Example results server could be used. This is helps to fulfil \cref{Req:Documentation}.

\subsection{Videogular Questions}
\label{Subsection:Videogular Questions in example}

There are several tests written for the \gls{vgQuestions} plugin. These use Jasmine, and can be run in an automated way using Karma.

These tests exercise the \gls{webworker} component of the \gls{vgQuestions} plugin over the message passing interface. A script is used, which contains messages to send, and expected responses. A test passes if the script runs as expected with no missing or extra messages.

The examples created in the Videogular Questions Example repository use individual or collections of features. These were used in the development of these features, and in the demonstration of these features to the customer.  These examples complement the Jasmine tests, as they exercise the display elements of the plugin.

There was no user experience testing done directly as part of the project, as the client had reservations about user experience testing due to the toolkit style nature of the project. Shameem performed some user experience testing for the Videogular Questions Example site and these were discussed in a meeting with both Shameem and the client \autoref{subsection:Meeting10Nov}. The consensus was that while the results were informative, they were outside the scope of this project.

\subsection{Videogular Cuepoints}
\label{Subsection:Videogular Cuepoints in example}
\gls{Videogular} Cuepoints was integrated into the example site, and configured to mark the times at which pop ups were displayed. The issue where marks could only be displayed once the video began to play (as described in \autoref{Subsection:vgCuepoints}) was the only problem encountered.

\subsection{Example Results Server}
\label{Subsection:Example Results Server in example}

For the example results server there was a small set of functions to test with a small number of possible inputs and a well defined set of responses. This is, therefore, well suited to individual unit tests.

Flask\footnote{\url{http://flask.pocoo.org/}} is a python web application framework that allows you to create web applications with simple routing patterns in python.

The Flask library had a test client and recommended test skeleton\footnote{\url{http://flask.pocoo.org/docs/0.10/testing/}} which was made use of to run the unit tests. This used the \lstinline|unittest| standard python library which meant it was easy to test with but also allowed calling of methods by simulating HTTP requests.

The testing was split into 6 areas to test the main components of the application: cross-origin resource sharing, routing, database setup, voting, getting results and load testing.

\subsubsection{Cross-Origin Resource Sharing tests}

One of the main requirements by the customer was that these units should be able to be accessed via \gls{REST} calls (see \cref{Req:Server architecture}). In addition the requirement stated that there should be no reliance that these servers are on the same host. To ensure that these \gls{REST} calls will not fail Cross-Origin Resource Sharing (CORS) headers were implemented as discussed in \autoref{Section:Modular Approach}.

This checks to ensure that the CORS header is correctly sent in the HTTP reply. If this is not set the web browser will likely reject the loading of the page and the application will fail.

In addition, it ensures that the response is the one expected and not an error state to ensure that the web application is also sending the correct content.

\subsubsection{Routing tests}

The routing tests are to ensure that the application correctly starts and is available.

If this fails to load up the testing URL which returns "Hello World!" then it is unlikely to be able to perform some of the more complex functions.

\subsubsection{Database Setup tests}

Before the application can be used it needs have its database set up. This is performed by accessing `/setup'. This set of unit tests test setting up a database and ensure that if this URL is visited twice, it successfully detects that the database is already set up and does not recreate it. 

This is important to ensure that the database is set up correctly and that when setup is visited again data in the database is not lost.

\subsubsection{Voting tests}

These tests try a number of different ways to vote by sending a number of different formats of invalid and valid data to check if the application correctly deals with the data. All return codes and responses are checked to ensure no invalid vote is accepted or valid one rejected.

\subsubsection{Getting results tests}

A number of valid votes are constructed and then sent into the system. Then these are attempted to be retrieved. The returned values are checked to ensure that they have not been corrupted. This submits one and multiple votes to ensure that all votes are correctly collated and returned.

If the ability to vote does not work then this will fail as it relies on being able to put votes into the system.

\subsubsection{Load testing}

For the load testing Locust was used and three access patterns were chosen to test the example results server:

\begin{itemize}
\item Visiting the root page - This is to determine that the webserver is still responding to basic HTTP requests
\item Voting - This is to test that the voting functionality is still accessible
\item Requesting data - This is to test that the server is still able to return data
\end{itemize}

These three URLs were visited repeatedly by the user clients over a period of time.

\begin{table}
\caption{Time taken for each request to complete in milliseconds while stress testing the example results server}
\begin{tabular}{l c  c c c c c c c c }
\hline 
& & \multicolumn{8}{c}{Percentile} \\
Name & Requests & 50\% & 75\% & 80\% & 90\% & 95\% & 98\% & 99\% & 100\% \\ 
\hline 
GET / & 535 & 5 & 50 & 67 & 110 & 170 & 240 & 360 & 601 \\ 
\hline 
GET /results/*/* & 1599 & 6 & 26 & 46 & 100 & 136 & 233 & 326 & 326 \\ 
\hline 
POST /vote & 1129 & 120 & 150 & 170 & 210 & 250 & 360 & 460 & 638 \\ 
\hline 
Total / Average & 3263 & 32 & 110 & 120 & 160 & 210 & 270 & 400 & 638 \\ 
\hline 
\end{tabular}
\label{Table:stress_testing_results_server}
\end{table}

\autoref{Table:stress_testing_results_server} shows the time taken for a response to be handled and returned.

Here you can see that the time taken for result requests takes less than 6 milliseconds 50\% of the time and all requests were dealt with in at most 326 milliseconds. The voting \gls{REST} point takes slightly longer as it needs to store data to the sqlite database but still took at most 638 milliseconds. All these tests were performed when 50 Locust users were using the website continuously and therefore it was agreed that the performance should scale. This is because one user will only ever make one vote and one results request per question and this tested for 50 users continuously making these requests.

Although this is only an example this ties in with \cref{Req:Scalability} that the results server should perform normally when a lecture room full of people is using it. The testing shows that the application is able to cope with 50 people continuously voting and therefore should be able to cope with a much less strenuous 300 people casting votes over a period of several minutes.

There is also scope to move the back-end database to something that is purely in memory to further increase the speed, as the major factor will be writing the data back to the hard disk.

\subsection{Accessibility}
\label{Subsection: vqe accessibility}

The \gls{Videogular} Questions Example was mostly accessible by the WCAG 2.0 standards (see \autoref{table: vqe conformance}). However the lack of alternatives for the video and the cuepoints using only colour may cause issues for some users.

The lack of captions or signed versions of the videos was down to the videos used so not was an issue with the page overall as it is a proof-of-concept.

A textual alternative was not provided because the idea is to integrate this into Synote which provides a transcript (see \autoref{Section:Synote}). Thus, this feature would be duplicated.

The cuepoints are only conveyed through the changing of colour on the scrub bar. In future extra behaviour could be added to these to make them accessible through other means.

As a proof-of-concept this site showed that it is possible to create an accessible questions overlay when combined with Synote.

\subsection{Deliverable signoff tests}

These tests in \autoref{table:vgQuestions Deliverable signoff tests} were presented to the client as the deliverable items for the Videogular Questions and Videogular Cuepoint plugins.

These tests were ran on the Videogular Questions Example website. This had examples of all types of questions and therefore was used as a deliverable for the customer.

\begin{center}
\begin{longtable}{|L{0.9}|L{0.03}|} 
\caption{\label{table:vgQuestions Deliverable signoff tests}vgQuestions Example Deliverable signoff tests} \\
\hline \textbf{Detail of test} & \rot{\textbf{Pass }}\\ \hline
\endfirsthead
\hline \textbf{Detail of test} & \rot{\textbf{Pass }}\\ \hline \endhead
\multicolumn{2}{c}{Continues on the next page...} \endfoot
\endlastfoot
Single Question provides radio buttons to select at most one answer. & \CheckmarkBold \eoline
Question Multiple provides checkboxes to select one or more results. The min/max values should be shown and \keys{Submit} should be disabled if these requirements are not met. & \CheckmarkBold \eoline
Questions Star should allow you to click on the stars and select one rating. The Min/Max values should be shown and \keys{Submit} should be disabled if these requirements are not met. & \CheckmarkBold \eoline
Questions Text should allow you to enter a textual value before submitting. & \CheckmarkBold \eoline
Question Range should present a slider which allows you to choose a range of values. The min/max values should be shown on the slider. & \CheckmarkBold \eoline
The tabs Single Question, Question Multiple, Question Stars, Question Text, Question Range all should show one demonstration of the specific question type. & \CheckmarkBold \eoline
The poll simple example should show an example poll. & \CheckmarkBold \eoline
Caesar Example should have Videogular Cuepoints enabled so you will be able to see where the questions will be shown. & \CheckmarkBold \eoline
Caesar Example's second question at the end of the example will have a poll to see the results of all those who have answered the question. & \CheckmarkBold \eoline
\end{longtable}
\end{center}

The pass marks show each test was tested by the developers on this project and was successful, and also accepted by the client after review of the deliverable report.


\section{Example Analytics Server}
\label{Subsection:Analytics server in example}

The analytics back-end server receives the \gls{REST} calls from the \gls{Videogular} Analytics plugin and is then able to process them. For this example implementation it was chosen not to store the data but forward it to the front-end. This will demonstrate that the Videogular Analytics plugin is able to make \gls{REST} calls. Storage could be used to collate the information for later recall.

\subsection{Cross-Origin Request testing}

\cref{Req:Standalone} states that each plugin should operate in a standalone manner. It needed to be ensure that the example server and analytics plugin works correctly when performing cross origin requests. To permit this on the example server testing code has been included to permit all cross origin requests, see \autoref{code:analyticsCrossOrigin}.

\begin{lstlisting}[language=javascript,caption={Code showing appending Cross-Origin headers to all responses}, label={code:analyticsCrossOrigin}]
app.all('*', function(req, res, next) {
	res.header("Access-Control-Allow-Origin", "*");
	res.header("Access-Control-Allow-Headers",
	           "Origin, X-Requested-With, Content-Type, Accept");
	next();
});
\end{lstlisting}

To test that the analytics worked cross-domain the analytics example site and the Videogular Analytics plugin were set up on two different testing servers. This was able to correctly pass data between the two and therefore passed cross-origin testing.

\subsection{Reloading processed data}
\label{Section:reloading processed data}

The back-end acted as a testing platform for the front-end. A large set of data was recorded from the Videogular Analytics plugin and saved to allow for repeated testing of the same pieces of data. The back-end plugin has a tool to specify a data file that has already been processed to be loaded.

Any front-end client connecting to the back-end \gls{Videogular} Analytics server will be sent all events in the loaded piece of data. This allows the front-end website to be quickly tested with a set of data that encompasses all features of the \gls{Videogular} Analytics plugin. This data set allows comparison between a known valid set of data and data that \gls{Videogular} Analytics creates allowing you to easily identify issues that occur.

This helps to faster identify cases where \gls{Videogular} Analytics is not conforming to the \gls{API} standard.

\section{Example Analytics Front-End}

The example analytics front-end loads up events from the \gls{Videogular} Analytics plugin. These events are obtained by a websocket to example analytics server. In the current model the example analytics server acts as an intermediary between this and the \gls{Videogular} Analytics plugin that can serve additional events as described in \autoref{Section:reloading processed data}.

This front-end tests the \gls{Videogular} Analytics plugin to ensure that the data received is not missing or corrupting events. This is done by loading the data into different formats and processing it. The data processing assumes a number of standard data elements expected from the Videogular Analytics based on the \gls{API} (\autoref{Table:analytics_api}).

\subsection{Videogular Heatmaps}
\label{Subsubsection:Videogular Heatmaps in example}
%Heatmap, user stories, run through

One of the analytics used in the example site is the \gls{Videogular} Heatmap. When running an instance of \gls{Videogular} Questions Example this can be used to dynamically update the heat map in the example analytics page (see \autoref{Figure: Heat map display}).

\begin{figure}[h]
	\centering
		\includegraphics[scale=0.4]{../figures/heatmapDisplay.png}
	\caption{\label{Figure: Heat map display} The heat map has dynamically updated}
\end{figure}

This is used to test that the \gls{Videogular} Analytics plugin is able to properly export the timing data. The processing on the client side matches up start and end marks. These marks shown in \autoref{Figure:Analytics time periods} are then further processed to produce the data that the analytics can use.

If these are not able to be processed then this indicates an issue with the events being sent from the \gls{Videogular} Analytics plugin. If they are able to be sent but the heat map does not display then there is an issue with the heat map.

Running this against the predefined data is used as a regression test for \gls{Videogular} Heatmap to ensure it is working correctly.

By running an instance of the Videogular Questions Example site and voting with the analytics end-to-end testing is performed as this data is passed through a number of the plugins to reach the Analytics front-end to be loaded by the \gls{Videogular} Heatmap plugin.

\begin{figure}[h]
	\centering
		\includegraphics[scale=0.4]{../figures/analytics_time_periods.png}
	\caption{\label{Figure:Analytics time periods} Analytics events after being processed into viewer time periods}
\end{figure}

The added feature of displaying the frequencies was also tested. These displayed well and matched the frequencies represented by the colours used. A screen reader could still access the data as it was declared in hidden text.

\subsection{Videogular Analytics integration with Videogular Questions}

To test integration with \gls{vgQuestions} a set of dynamic graphs were included that update as events about question results are sent. \autoref{Figure:Analytics Question Graphs} shows an example of the results of questions that have been sent from the plugin.

\begin{figure}[h]
	\centering
		\includegraphics[scale=0.8]{../figures/analytics_graphs.png}
	\caption{\label{Figure:Analytics Question Graphs} Graph showing results of questions sent from Videogular Analytics}
\end{figure}

If the question data is not being received this means that the \gls{Videogular} Analytics plugin has not been able to properly access the \gls{vgQuestions} events. This test was used as a regression test to ensure that the Videogular Analytics plugin is correctly accessing all published analytics events and forwarding them to the \gls{REST} server.

\subsection{Accessibility}

The example analytics site generally conformed to the \gls{WCAG} 2.0 standards (see \autoref{table: va conformance}). The non-conformances found were a lack of alternatives for the video and graphs.

The video issues were the same as for the Videogular Questions Example site (see \autoref{Subsection: vqe accessibility}). However the cuepoints were not used. The heat maps had a textual option that could be displayed which gave additional methods for conveying the information.

Using graphs has created many accessibility issues for visually impaired or cognitively impaired users as the data is not described in text. This would be an important consideration for any developer using the plugins in their own site. As a proof-of-concept the example site purely shows that graphs could be used as a method of automatically producing a visual representation of the data.

\subsection{Deliverable signoff tests}

The tests in table \autoref{table:vgAnalytics Deliverable signoff tests} were presented to the client as the deliverable items for the example analytics tool.

These tests were ran on the analytics tool example using preloaded data so that there was had a repeatable set of data that was always valid.

\begin{center}
\begin{longtable}{|L{0.9}|L{0.03}|} 
\caption{\label{table:vgAnalytics Deliverable signoff tests}vgAnalytics Deliverable signoff tests} \\
\hline \textbf{Detail of test} & \rot{\textbf{Pass }}\\ \hline
\endfirsthead
\hline \textbf{Detail of test} & \rot{\textbf{Pass }}\\ \hline \endhead
\multicolumn{2}{c}{Continues on the next page...} \endfoot
\endlastfoot
All events are listed at the top of the events log page, including the type and details of the events & \CheckmarkBold \eoline
The sections that the users have watched are correctly calculated and shown at the bottom & \CheckmarkBold \eoline
Watched video segments are calculated correctly and show how many times all users have viewed a section. & \CheckmarkBold \eoline
The results page shows the user responses. & \CheckmarkBold \eoline
The results page also shows correct answers and whether users rewatched sections. & \CheckmarkBold \eoline
The ``\% watched by correct answers'' and ``Time watched by correct answers'' graphs show two ways you could represent the data in a different way. & \XSolidBrush \eoline
\end{longtable}
\end{center}

Here the final requirement was queried by the customer. The example graphs shown did not have an vertical axis. This was fixed by adding an axis after the customer meeting however during the meeting this did not affect signoff since it was an example of possible graphs that could be created.

\section{Authoring Tool}

The authoring tool is primary deliverable as it is the only item that is designed as a tool and not as a proof-of-concept example site deliverable. Therefore the primary types of testing are deliverable signoff tests and accessibility testing.

\subsection{Accessibility}
\label{Section:Testing_Authoring_tool_accessibility}

The authoring tool was found to be generally accessible conforming to most of the \gls{WCAG} 2.0 standards (see \autoref{table: authoring tool conformance}). The areas that were lacking were providing textual alternatives and error checking.

There were no alternatives provided for the video. However as this would be used with Synote transcripts could be used from there. In future, support for extra accessibility features such as transcripts, captions or signed versions could be included in the authoring tool.

In creating the questions there is no help provided for error checking. This is because it was decided not to narrow the options available to the users. In the occurrences where errors are identified these are generally highlighted by surrounding the component with a red border. More information on how the system would be used would be required in order to create helpful error messages.

Two keyboard accessibility issues were found, both of which were due to browser bugs. The first was that Google Chrome and Chromium did not allow multiple items to be selected in a \texttt{\textless select multiple\textgreater} element using only the keyboard. During the course of the project, a fix was made to Google Chrome and Chromium which allowed multiple \textit{adjacent} items to be selected with the keyboard \citep{ChromiumMultipleSelectBug}, which is an improvement but still not a complete fix.

The second keyboard accessibility issue was that collapsible section headings caused ``focus loops'' in Mozilla Firefox, where pressing the Tab key from the last button on the heading caused the focus to move back to the heading itself, preventing the user from focussing any content after the heading. This was reported on the Mozilla bug tracker \citep{FirefoxFocusLoopBug} (see \cref{App:Firefox Bug Report}), where another user pointed out that the HTML causing the bug was invalid. The HTML was rewritten to be valid, which fixed the issue.

Another issue found was in the use of the \texttt{\textless select\textgreater} element as the colour of highlighted options is not editable in the \gls{CSS}. To solve this issue the elements would all need rewriting to take a completely different structure to allow the \gls{CSS} to modify the colours used.

Accessibility in the authoring tool was good but there are improvements that could be made in future including adding support for accessibility features such as subtitles, providing error correction support and rewriting the \texttt{\textless select\textgreater} elements to allow editing of colours.

\subsection{Deliverable signoff tests}

The tests in \autoref{table:Authoring tool Deliverable signoff tests} were presented to the client as the deliverable items for the authoring tool.

\begin{center}
\begin{longtable}{|L{0.9}|L{0.03}|}
\caption{\label{table:Authoring tool Deliverable signoff tests}Authoring tool Deliverable signoff tests} \\
\hline \textbf{Detail of test} & \rot{\textbf{Pass }}\\ \hline
\endfirsthead
\hline \textbf{Detail of test} & \rot{\textbf{Pass }}\\ \hline \endhead
\multicolumn{2}{c}{Continues on the next page...} \endfoot
\endlastfoot
Pressing \keys{Export} will provide you with a \gls{DF} which can be used. & \CheckmarkBold \eoline
Pressing \keys{Preview} will reset the video back to the start, load the questions into the preview, and begin playing & \CheckmarkBold \eoline
All 5 question types are implemented and, as suggested, the Single choice question (with radio buttons) is merged with the Multiple choice question. & \CheckmarkBold \eoline
A question set is able to be added at a specific time. & \CheckmarkBold \eoline
Once a single question has been added the \keys{Preview} button is used to ensure this is shown properly & \CheckmarkBold \eoline
Each question type is tested along with common options and displays as expected. & \CheckmarkBold \eoline
Skipping is tested for one question and works as expected. & \CheckmarkBold \eoline
When checked, the record button sends the poll data to the poll server. & \CheckmarkBold \eoline
A full walkthrough is attempted, all question types are able to be added and previewing, and exporting works & \CheckmarkBold \eoline
\end{longtable}
\end{center}

The pass marks show each test was tested by the team and was successful, and also accepted by the client after review of the deliverable report.

\section{Summary}

Working in an agile manner meant that each week work was presented to the client. This meant that the primary types of testing are ensuring functionality works through integration tests and the final deliverable reports. The example websites have been created to demonstrate that the plugin works and to allow testing of them in the environment they are planned to be used in.

Some repositories have been tested using a unit testing strategy(\autoref{Section:Testing_unittesting}) which is focussed on ensuring that regression tests are ran and detection of errors before committing code. For these repositories the master branch should always build and function correctly. This has allowed meetings with the client to always demonstrate working projects while at any stage of development.

The analytics front-end server is an example of integration testing performed. Here \gls{Videogular}, Videogular Analytics, and Videogular Heatmap is used to demonstrate a number of statistics about how a user is using the interface. There are tests for each of these modules but identified a number of issues that occurred after linking these plugins together. Videogular Questions, Videogular Analytics, and results server have been tested in this manner using the Videogular Examples website.

In server applications that require a fast response load testing has been performed to ensure that performance degradation would not affect a users experience. This has been shown (\autoref{Table:stress_testing_results_server}) to cause any user to have high latency and suggestions have been made to improve it if needed.

These tests have demonstrated to the client that this has been thoroughly tested and they have signed off the deliverable report (\cref{App:Deliverable Report}) presented to them. The documentation provided also includes recommended practices for continued work to satisfy \cref{Req:Documentation}.